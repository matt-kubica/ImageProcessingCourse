{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bY2cnBzEoHck"
   },
   "source": [
    "# Klasyfikacja znaków drogowym z wykorzystaniem DCNN\n",
    "\n",
    "Uwaga. Ćwiczenie powstało w oparciu o następujący [tutorial](https://www.pyimagesearch.com/2019/11/04/traffic-sign-classification-with-keras-and-deep-learning/).\n",
    "Osoby zainteresowane tematem - moim zdaniem (Tomasz Kryjak) powinien to być każdy z Państwa -- zachęcam do prześledzenia tego dokumentu bardziej szczegółowo.\n",
    "\n",
    "Inne podobne tutoriale:\n",
    "- https://chsasank.github.io/keras-tutorial.html\n",
    "- https://towardsdatascience.com/building-a-road-sign-classifier-in-keras-764df99fdd6a\n",
    "\n",
    "\n",
    "To krótkie ćwicznie nie zastąpi pełnego kursu z AI, ale powinno pozwolić poznać podstawowe etapy związane z inżynierskim wykorzystaniem głębokich konwolucyjnych sieci neuronowych (DCNN - Deep Convolutional Neural Network).\n",
    "\n",
    "## Założenia (v 1.0)\n",
    "\n",
    "Poniższy notatnik jest kompletny i \"do uruchomienia\".\n",
    "Jedyne wyzwania (niekoniecznie trywialne) to sprawy techniczne.\n",
    "W przyszłości będzie to ulegało zmianom.\n",
    "\n",
    "Uwaga. Czas obliczeń może być znaczny, ale można go wykorzystać np. na czytanie wskazanego tutoriala, tudzież analizę kodu :).\n",
    "\n",
    "## Instalacja, sprawy techniczne\n",
    "\n",
    "Notatnik do działania potrzebuje pakietów:\n",
    "- OpenCV\n",
    "- NumPy\n",
    "- scikit-learn\n",
    "- scikit-image\n",
    "- imutils\n",
    "- matplotlib\n",
    "- TensorFlow 2.0 (CPU or GPU)\n",
    "\n",
    "Opcje uruchomienia są dwie:\n",
    "- Google Colaboratory - tam wszystko jest zainstalowane + mamy zasoby obliczeniowe, ale trzeba nieco pokombinować z dostarczeniem danych. Opis poniżej. Jest też opcja rekomendowana.\n",
    "- lokalnie (instalacja pakietów via *pip* lub poprzez PyCharm).\n",
    "\n",
    "Dodatkowo należy pobrać bazę danych [GTSRB](https://drive.google.com/file/d/1EQ-tyVHIdVaa4_1bob1zv8zgaVmvyyqV/view?usp=sharing) (German Traffic Sign Recognition Benchmark) (300 MB).\n",
    "Baza zawiera ponad 50000 obrazków dal 43 klas znaków.\n",
    "Ma też dwie istotne wady:\n",
    "- różna liczba przykładów z poszczególnych klas (od 180 do ponad 2000),\n",
    "- część znaków stanowi duże wyzwanie (słaba jakość, kontrast) - uczciwie mówiąć, to niektóre trudno rozpoznać.\n",
    "\n",
    "Nie wchodząc w zbytnie szczegóły.\n",
    "Sieć DCNN \"się uczy\" (jest uczona) na podstawie przykładów.\n",
    "Podobnie jak uczymy się my.\n",
    "I teraz np. jeśli rozwiążemy 10 zadań nt. całek i 100 na temat pochodnych, to co na egzaminie co wyjdzie lepiej ?\n",
    "Stąd tego typu dysproporcja stanowi problem.\n",
    "Słaba jakość zdjęć też utrudnia uczenie.\n",
    "\n",
    "\n",
    "W przypadku przetwarzania lokalnego bazę należy rozpakować.\n",
    "\n",
    "\n",
    "## Informacje wstępne DCNN \n",
    "\n",
    "Co to jest sieć DCNN (Deep Convolutional Neural Network) ?\n",
    "\n",
    "Model naśladujący działanie ludzkiego mózgu (tu konkretnie sposobu przetwarzania informacji wizyjnej).\n",
    "Przedstawienie całej teorii w notatniku nie jest specjalnie wygodne -- zainteresowane osoby odsyłam do obszernej literatury.\n",
    "Bez wchodzenia w szczegóły, sieć można traktować jako czarną skrzynkę, która na wejściu dostaje obraz, a na wyjściu wyniki (klasy obiektów).\n",
    "\n",
    "Przy czym sieć trzeba nauczyć.\n",
    "W uproszczeniu proces polega na tym, że prezentujemy sieci obraz, otrzymujemy jakiś wynik, porównujemy go z pożądanym i wg. specjalnego algorytmu modyfikujemy parametry sieci (tzw. wsteczna propogacja błędu).\n",
    "Uwaga. To jest tzw. uczenie nadzorowane (z nauczycielem).\n",
    "Istnieje tez uczenie bez nauczyciela (sieci samouczące) oraz ze wspomaganiem (reinforcement learing).\n",
    "Szczególnie to drugie jest bardzi ciekwe - warto sobie o tym poczytać.\n",
    "To ta metodologia stoi za sukcesami AlphaGo (w grze Go), czy w Starcrafta.\n",
    "\n",
    "Mając nauczoną sieć, można przeprowadzić tzw. wnioskowanie (ang. *inference*).\n",
    "\n",
    "Warto też wiedzieć, że sieci konwolucyjne to jest jedna z możliwości, dedykowana do obrazów.\n",
    "Dla innych zagadnienień stosowane są inne modele.\n",
    "Ponadto w ramach samych DCNN występuje wiele różnych rozwiązań, choć są one zbudowane z mniej więcej podobnych \"klocków\".\n",
    "\n",
    "Źródła dodatkowych informacji:\n",
    "- https://en.wikipedia.org/wiki/Convolutional_neural_network\n",
    "- https://d2l.ai/\n",
    "- kursy na Coursera\n",
    "- a tak na prawdę to jest tego bardzo dużo. \n",
    "\n",
    "\n",
    "## Co my dzisiaj zrobimy ?\n",
    "\n",
    "Przejdziemy przez następujące kroki:\n",
    "- utworzenie modelu (definicja architektury sieci),\n",
    "- implementacja funkcji do przygotowania zbioru uczącego,\n",
    "- przygotowanie danych,\n",
    "- uczenie,\n",
    "- analiza wyników uczenia,\n",
    "- testy: wnioskowanie.\n",
    "\n",
    "Bardziej szczegółowe komentarze w tekście.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "fN5Jt-mooHcq"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Model created\n"
     ]
    }
   ],
   "source": [
    "# Definicja architektury sieci\n",
    "\n",
    "# Potrzebne biblioteki\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.metrics import classification_report\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Siec do klasyfikacji\n",
    "# Składa się z:\n",
    "# 5 warstw konwolucyjnych - Conv2D, po nich funkcja aktywacji (relu), normalizacja oraz podpróbkowanie (zmiana rozdzielczości).\n",
    "# 2 warstwy tzw. w pełni połączonych\n",
    "\n",
    "# Uwaga. Sieć ta jest zbliżona do rozwiązana AlexNet (https://en.wikipedia.org/wiki/AlexNet). \n",
    "# Natomiast sam AlexNet to jedna z pierwszych (a na pewno najbardziej znanych) sieci konwolucyjnych.\n",
    "# To m.in. sukses tego rozwiązania w konkursie  ImageNet Large Scale Visual Recognition Challenge.\n",
    "# Artykluł ja opisujący ma ponad 70000 cytowań.\n",
    "\n",
    "class TrafficSignNet:\n",
    "    @staticmethod\n",
    "    def build(width, height, depth, classes):\n",
    "\n",
    "        model = Sequential()\n",
    "        inputShape = (height, width, depth)\n",
    "        chanDim = -1\n",
    "\n",
    "        # CONV => RELU => BN => POOL\n",
    "        model.add(Conv2D(8, (5, 5), padding=\"same\", input_shape=inputShape))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # first set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(16, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # second set of (CONV => RELU => CONV => RELU) * 2 => POOL\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(Conv2D(32, (3, 3), padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(axis=chanDim))\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "        # first set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # second set of FC => RELU layers\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(128))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # Klasyfikator softmax\n",
    "        model.add(Dense(classes))\n",
    "        model.add(Activation(\"softmax\"))\n",
    "\n",
    "        return model\n",
    "\n",
    "print(\"[INFO] Model created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vXvPKNQMoHcs",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Function defined\n"
     ]
    }
   ],
   "source": [
    "# Funkcja do przygotowania obrazów\n",
    "\n",
    "def load_split(basePath, csvPath):\n",
    "    # Inicjalizacja list dla danych i etykiet (klas znaków)\n",
    "    data = []\n",
    "    labels = []\n",
    "\t# Wczytanie zawartości pliku CVS z opisem danych, pominięcie pierwszej linii.\n",
    "    rows = open(csvPath).read().strip().split(\"\\n\")[1:]\n",
    "    # Wymieszanie przykładów uczących\n",
    "    random.shuffle(rows)\n",
    "\n",
    "    # Pętla po przykładach uczących\n",
    "    for (i, row) in enumerate(rows):\n",
    "\t    # Wypisanie informacji co 1000 przykładów\n",
    "        if i > 0 and i % 1000 == 0:\n",
    "           print(\"[INFO] processed {} total images\".format(i))\n",
    "\n",
    "        # Dla danego rzędu pozyskujemy etykietę (label) oraz ścieżkę do pliku\n",
    "        (label, imagePath) = row.strip().split(\",\")[-2:]\n",
    "\t  \t# \"Skompletowanie\" ścieżki i wczytanie obrazu\n",
    "        imagePath = os.path.sep.join([basePath, imagePath])\n",
    "        image = io.imread(imagePath)\n",
    "        # Przeskalowanie do rozmiaru 32x32 i poprawa kontrastu metodą CLHAE (Contrast Limited Adaptive Histogram Equalization)\n",
    "        image = transform.resize(image, (32, 32))\n",
    "        image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "        # Dodanie obrazka i etykiekt do listy\n",
    "        data.append(image)\n",
    "        labels.append(int(label))\n",
    "\n",
    "    # Konwersja danych i etykiet na tablice NumPy\n",
    "    data = np.array(data)\n",
    "    labels = np.array(labels)\n",
    "\t  # Zwracamy dane i etykiety (w formie krotki)\n",
    "    return (data, labels)\n",
    "\n",
    "print(\"[INFO] Function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "tbbFlWRnoHct",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading training and testing data...\n",
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n",
      "[INFO] processed 13000 total images\n",
      "[INFO] processed 14000 total images\n",
      "[INFO] processed 15000 total images\n",
      "[INFO] processed 16000 total images\n",
      "[INFO] processed 17000 total images\n",
      "[INFO] processed 18000 total images\n",
      "[INFO] processed 19000 total images\n",
      "[INFO] processed 20000 total images\n",
      "[INFO] processed 21000 total images\n",
      "[INFO] processed 22000 total images\n",
      "[INFO] processed 23000 total images\n",
      "[INFO] processed 24000 total images\n",
      "[INFO] processed 25000 total images\n",
      "[INFO] processed 26000 total images\n",
      "[INFO] processed 27000 total images\n",
      "[INFO] processed 28000 total images\n",
      "[INFO] processed 29000 total images\n",
      "[INFO] processed 30000 total images\n",
      "[INFO] processed 31000 total images\n",
      "[INFO] processed 32000 total images\n",
      "[INFO] processed 33000 total images\n",
      "[INFO] processed 34000 total images\n",
      "[INFO] processed 35000 total images\n",
      "[INFO] processed 36000 total images\n",
      "[INFO] processed 37000 total images\n",
      "[INFO] processed 38000 total images\n",
      "[INFO] processed 39000 total images\n",
      "[INFO] processed 1000 total images\n",
      "[INFO] processed 2000 total images\n",
      "[INFO] processed 3000 total images\n",
      "[INFO] processed 4000 total images\n",
      "[INFO] processed 5000 total images\n",
      "[INFO] processed 6000 total images\n",
      "[INFO] processed 7000 total images\n",
      "[INFO] processed 8000 total images\n",
      "[INFO] processed 9000 total images\n",
      "[INFO] processed 10000 total images\n",
      "[INFO] processed 11000 total images\n",
      "[INFO] processed 12000 total images\n",
      "[INFO] saving training and testing data...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Pobrani pliku z nazwami znakow\n",
    "if not os.path.exists(\"signnames.csv\") :\n",
    "    !wget https://raw.githubusercontent.com/vision-agh/poc_sw/master/14_TSR_DCNN/signnames.csv --no-check-certificate\n",
    "\n",
    "# Przygotowanie danych\n",
    "\n",
    "# Jeśli ktoś używa Google Colab\n",
    "# 1. Wgrać plik gtsrb.zip na dysk googla\n",
    "# 2. Podmontować dysk.\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "\n",
    "# # 3. Zainstlować zip i rozpakować plik\n",
    "# # Uwaga ustawić scieżke\n",
    "# !apt install unzip\n",
    "# !unzip '/content/drive/MyDrive/dydaktyka/POC/Python/gtsrb.zip'\n",
    "\n",
    "\n",
    "# Jeśli ktoś pracuje lokalnie, to trzeba tu ustawić ścieżkę.\n",
    "# Ścieżka do danych\n",
    "dataset = \"gtsrb\"\n",
    "\n",
    "# Wczytanie nazw etykiet\n",
    "labelNames = open(\"signnames.csv\").read().strip().split(\"\\n\")[1:]\n",
    "labelNames = [l.split(\",\")[0] for l in labelNames]\n",
    "\n",
    "# Ustawienie ścieżki do zbioru uczącego i testowego\n",
    "trainPath = os.path.sep.join([dataset, \"Train.csv\"])\n",
    "testPath = os.path.sep.join([dataset, \"Test.csv\"])\n",
    "\n",
    "# Wczytanie danych uczących i testowych (dość długo trwa).\n",
    "print(\"[INFO] loading training and testing data...\")\n",
    "(trainX, trainY) = load_split(dataset, trainPath)\n",
    "(testX, testY) = load_split(dataset, testPath)\n",
    "\n",
    "# Przeskalowanie danych do zakresu [0, 1]\n",
    "trainX = trainX.astype(\"float32\") / 255.0\n",
    "testX = testX.astype(\"float32\") / 255.0\n",
    "\n",
    "# Zakodowanie etykiet danych uczących i testowych w formacie *one-hot* (z całego wektora tylko jedna wartość to 1, reszta 0).\n",
    "# To wprost koresponduje z wyjściem z sieci (warstwa softmax), gdzie otrzymujemy wektor w długości takiej, ile mamy klas (tu 43) i wyszukujemy w nim maksimum.\n",
    "numLabels = len(np.unique(trainY))\n",
    "trainY = to_categorical(trainY, numLabels)\n",
    "testY = to_categorical(testY, numLabels)\n",
    "\n",
    "# Zapis danych uczących i testowych do pliku (żeby tego ew. nie powtarzać), jak coś na dalszym etapie pójdzie nie tak.\n",
    "with open('train_test.pickle', 'wb') as f:\n",
    "    pickle.dump([trainX, trainY, testX, testY], f)\n",
    "\n",
    "print(\"[INFO] saving training and testing data...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBqJ4buypzrM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "bTuFQ4cXoHcu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "[INFO] model ready to learn...\n"
     ]
    }
   ],
   "source": [
    "# Przygotowanie do uczenia modelu\n",
    "\n",
    "# Liczba etykiet\n",
    "numLabels = trainY.shape[1]\n",
    "\n",
    "# Liczba epok (iteracji algorytmu uczenia)\n",
    "NUM_EPOCHS = 15\n",
    "\n",
    "# Współczynnik uczenia\n",
    "INIT_LR = 1e-3\n",
    "\n",
    "# Rozmiar \"wsadu\" do batch normalization (https://en.wikipedia.org/wiki/Batch_normalization)\n",
    "BS = 64\n",
    "\n",
    "# Wczytanie zbioru uczącego i testowego\n",
    "with open('train_test.pickle', 'rb') as f:\n",
    "    trainX, trainY, testX, testY = pickle.load(f)\n",
    "\n",
    "# Stworzenie obiektu do augumentacji danych\n",
    "# Co to jest augumentacja ? \n",
    "# Ogólnie jest to \"sztuczne\" zwiększenie liczebności zbioru uczącego.\n",
    "# Jak można się domyśleć po nazwach, tu obejmuje takie operacje jak: obrót, skalowanie, przesunięcie, czy zniekształcenie.\n",
    "aug = ImageDataGenerator(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=False,\n",
    "    vertical_flip=False,\n",
    "    fill_mode=\"nearest\")\n",
    "\n",
    "# Inicjalizacja optymalizatora oraz kompilacja modułu\n",
    "# Parameter LR (Learning Rate) mówi nam o tym, jak bardzo sieć się uczy (jak bardzo możemy zmienić paramtery z danym kroku).\n",
    "# Proszę zwrócić uwagę, że ustawia się również jego zanikanie (decay).\n",
    "# Upraszczając (znowu). W poszukiwaniu optimum lokalnego, w przestrzeni rozwiązań (bo do tego ostatecznie sprowadza się problem uczenia), na początu dopuszczamy duże przesunięcia, a z czasem coraz mniejsze.\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "opt = Adam(lr=INIT_LR, decay=INIT_LR / (NUM_EPOCHS * 0.5))\n",
    "model = TrafficSignNet.build(width=32, height=32, depth=3, classes=numLabels)\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])\n",
    "\n",
    "# Wyliczanie wag dla klas -> wskazanie dla modułu, że występuje problem z różną liczebnością zbioru uczącego.\n",
    "classTotals = trainY.sum(axis=0)\n",
    "classWeight = classTotals.max() / classTotals\n",
    "classWeightD = {x: classWeight[x] for x in range(0,classWeight.shape[0]) }\n",
    "\n",
    "print(\"[INFO] model ready to learn...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "AwqyyiSOoHcv",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] training network...\n",
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mateusz/.local/share/virtualenvs/ImageProcessingCourse-sKn4OmTE/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612/612 [==============================] - 38s 59ms/step - loss: 10.1169 - accuracy: 0.0628 - val_loss: 3.6025 - val_accuracy: 0.0657\n",
      "Epoch 2/15\n",
      "612/612 [==============================] - 36s 59ms/step - loss: 5.5377 - accuracy: 0.2929 - val_loss: 1.4395 - val_accuracy: 0.5341\n",
      "Epoch 3/15\n",
      "612/612 [==============================] - 39s 64ms/step - loss: 3.8845 - accuracy: 0.4601 - val_loss: 1.1472 - val_accuracy: 0.6176\n",
      "Epoch 4/15\n",
      "612/612 [==============================] - 38s 62ms/step - loss: 2.9726 - accuracy: 0.5661 - val_loss: 0.8275 - val_accuracy: 0.7354\n",
      "Epoch 5/15\n",
      "612/612 [==============================] - 41s 67ms/step - loss: 2.2820 - accuracy: 0.6443 - val_loss: 0.7216 - val_accuracy: 0.7643\n",
      "Epoch 6/15\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 1.8927 - accuracy: 0.6993 - val_loss: 0.7556 - val_accuracy: 0.7473\n",
      "Epoch 7/15\n",
      "612/612 [==============================] - 44s 72ms/step - loss: 1.6227 - accuracy: 0.7338 - val_loss: 0.5726 - val_accuracy: 0.8190\n",
      "Epoch 8/15\n",
      "612/612 [==============================] - 40s 66ms/step - loss: 1.4189 - accuracy: 0.7696 - val_loss: 0.4630 - val_accuracy: 0.8412\n",
      "Epoch 9/15\n",
      "612/612 [==============================] - 42s 69ms/step - loss: 1.2916 - accuracy: 0.7910 - val_loss: 0.4019 - val_accuracy: 0.8599\n",
      "Epoch 10/15\n",
      "612/612 [==============================] - 39s 63ms/step - loss: 1.1569 - accuracy: 0.8109 - val_loss: 0.3865 - val_accuracy: 0.8709\n",
      "Epoch 11/15\n",
      "612/612 [==============================] - 38s 63ms/step - loss: 1.0233 - accuracy: 0.8271 - val_loss: 0.4517 - val_accuracy: 0.8541\n",
      "Epoch 12/15\n",
      "612/612 [==============================] - 37s 60ms/step - loss: 0.9982 - accuracy: 0.8363 - val_loss: 0.3420 - val_accuracy: 0.8881\n",
      "Epoch 13/15\n",
      "612/612 [==============================] - 39s 64ms/step - loss: 0.9004 - accuracy: 0.8506 - val_loss: 0.3468 - val_accuracy: 0.8877\n",
      "Epoch 14/15\n",
      "612/612 [==============================] - 39s 63ms/step - loss: 0.8794 - accuracy: 0.8582 - val_loss: 0.3042 - val_accuracy: 0.8964\n",
      "Epoch 15/15\n",
      "612/612 [==============================] - 39s 64ms/step - loss: 0.8363 - accuracy: 0.8662 - val_loss: 0.3680 - val_accuracy: 0.8799\n",
      "[INFO] serializing network to '{}'... trafficsignnet.model\n",
      "INFO:tensorflow:Assets written to: trafficsignnet.model/assets\n",
      "[INFO] training network done\n"
     ]
    }
   ],
   "source": [
    "# Uczenie modelu\n",
    "# To może chwile trwać - ograniczyliśmy liczbę epok (iteracji procesu do 15).\n",
    "# Ogólnie jest to etap, który dobrze można akcelerować na GPU. \n",
    "# Przy czym lokalna konfiguracja TensorFlow/Keras do współpracy z GPU nie jest prosta (i na pewno trwa znacznie dłużej niż to uczenie :)).\n",
    "# W kolejnych epokach wyświetlają się wskaźniki\n",
    "# - accuracy - dokładność na zbiorze uczącym (ma rosnąć)\n",
    "# - loss - funkcja błędu dla zbioru uczącego (ma maleć)\n",
    "# - val_accuracy - dokładność dla zbioru walidacyjnego (ma rosnąć)\n",
    "# - val_loss - funkcja błędu  na zbiorze walidacyjnym (ma maleć)\n",
    "\n",
    "# Zbiory uczące i testowe są rozłączne, aby móc zaobserwować zjawisko \"przeuczenia\" modelu (ang. overfitting).\n",
    "# Najkrócej ujmując - jest to analogia nauki na pamięć. Model dobrze nauczy się danych uczących, a kiepsko będzie sobie radził na innych.\n",
    "\n",
    "\n",
    "\n",
    "print(\"[INFO] training network...\")\n",
    "H = model.fit_generator(\n",
    "    aug.flow(trainX, trainY, batch_size=BS),\n",
    "    validation_data=(testX, testY),\n",
    "    steps_per_epoch=trainX.shape[0] // BS,\n",
    "    epochs=NUM_EPOCHS,\n",
    "    class_weight=classWeightD,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Zapis sieci na dysk - coby nie trzeba drugi raz uczyć.\n",
    "print(\"[INFO] serializing network to '{}'... trafficsignnet.model\")\n",
    "model.save(\"trafficsignnet.model\")\n",
    "\n",
    "\n",
    "print(\"[INFO] training network done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdlFeBWYoHcw"
   },
   "source": [
    "## Ewaluacja\n",
    "\n",
    "Mając dany znak i wynik klasyfikacji mamy 4 możliwości:\n",
    "- TP (True Positive) - wynik klasyfikacji i stan faktyczny są zgodne,\n",
    "- FP (False Positive) - klasyfikacja wskazuje na znak X, ale stan faktyczny to znak Y,\n",
    "- FN (False Negative) - klasyfikacja wskazuje, że to nie znak X, a stan faktyczny to X,\n",
    "- TN (True Negative) - klasyfikacja wskazuje, że to nie znak X i to nie jest znak X.\n",
    "\n",
    "Na tej podstawie można konstruować wskaźniki:\n",
    "- precision = TP / (TP+FP)\n",
    "- recall = TP / (TP+FN)\n",
    "- f1 = 2 * precision*recall / (precision+recall)\n",
    "\n",
    "Szerszy opis na [Wikipedii](https://en.wikipedia.org/wiki/Sensitivity_and_specificity).\n",
    "\n",
    "Parametr *support* oznacza liczbę próbek.\n",
    "\n",
    "Warto na chwilę się zastanowić nad tym, co oznaczają wskaźniki.\n",
    "Precyzja (precision) będzie tym większa, im mniej będzie FP, czyli sytuacji, że znak Y będzie uznany X (należący do ewaluowanej klasy).\n",
    "Natomiast  (recall), będzie tym większa, im miej będzie FN, czyli sytuacji, że znak X (należący na rozpatrywanej klasy) będzie uznany co Y.\n",
    "\n",
    "Należy zwrócić uwagę, że:\n",
    "- w idealnym przypadku (brak błędów) oba powinny być 1,\n",
    "- są poniekąd przeciwstawne - wszystko zależy od tego, czy nasz klasyfikator jest mniej, czy bardziej restrykcyjny.\n",
    "- f1, jakoś średnia harmoniczna, łączy oba wskaźniki.\n",
    "\n",
    "\n",
    "Proszę jeszcze zwrócić uwagę na rysunek `train.png` - wyświetlić i zastanowić się co oznacza.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "RJ2o_xt9oHcw",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] evaluating network...\n",
      "                               precision    recall  f1-score   support\n",
      "\n",
      "         Speed limit (20km/h)       0.34      0.90      0.49        60\n",
      "         Speed limit (30km/h)       0.87      0.76      0.81       720\n",
      "         Speed limit (50km/h)       0.88      0.83      0.85       750\n",
      "         Speed limit (60km/h)       0.88      0.65      0.75       450\n",
      "         Speed limit (70km/h)       0.89      0.83      0.86       660\n",
      "         Speed limit (80km/h)       0.78      0.75      0.77       630\n",
      "  End of speed limit (80km/h)       0.94      0.88      0.91       150\n",
      "        Speed limit (100km/h)       0.67      0.86      0.76       450\n",
      "        Speed limit (120km/h)       0.73      0.89      0.80       450\n",
      "                   No passing       0.99      0.92      0.95       480\n",
      " No passing veh over 3.5 tons       0.97      0.97      0.97       660\n",
      " Right-of-way at intersection       0.78      0.96      0.86       420\n",
      "                Priority road       0.99      0.97      0.98       690\n",
      "                        Yield       0.99      0.99      0.99       720\n",
      "                         Stop       0.95      1.00      0.97       270\n",
      "                  No vehicles       0.96      0.87      0.91       210\n",
      "    Veh > 3.5 tons prohibited       0.94      1.00      0.97       150\n",
      "                     No entry       1.00      0.99      0.99       360\n",
      "              General caution       0.98      0.69      0.81       390\n",
      "         Dangerous curve left       0.94      1.00      0.97        60\n",
      "        Dangerous curve right       0.64      0.62      0.63        90\n",
      "                 Double curve       0.83      0.63      0.72        90\n",
      "                   Bumpy road       0.99      0.64      0.78       120\n",
      "                Slippery road       0.77      0.89      0.82       150\n",
      "    Road narrows on the right       0.75      0.93      0.83        90\n",
      "                    Road work       0.94      0.88      0.91       480\n",
      "              Traffic signals       0.93      0.92      0.92       180\n",
      "                  Pedestrians       0.33      0.57      0.42        60\n",
      "            Children crossing       0.79      0.93      0.86       150\n",
      "            Bicycles crossing       0.61      1.00      0.76        90\n",
      "           Beware of ice/snow       0.59      0.42      0.49       150\n",
      "        Wild animals crossing       0.92      0.97      0.94       270\n",
      "   End speed + passing limits       1.00      0.97      0.98        60\n",
      "             Turn right ahead       0.95      1.00      0.98       210\n",
      "              Turn left ahead       0.98      1.00      0.99       120\n",
      "                   Ahead only       1.00      0.97      0.98       390\n",
      "         Go straight or right       0.93      0.99      0.96       120\n",
      "          Go straight or left       0.98      0.97      0.97        60\n",
      "                   Keep right       0.99      0.99      0.99       690\n",
      "                    Keep left       1.00      0.97      0.98        90\n",
      "         Roundabout mandatory       0.66      0.97      0.79        90\n",
      "            End of no passing       1.00      0.83      0.91        60\n",
      "End no passing veh > 3.5 tons       0.75      0.86      0.80        90\n",
      "\n",
      "                     accuracy                           0.88     12630\n",
      "                    macro avg       0.86      0.87      0.86     12630\n",
      "                 weighted avg       0.89      0.88      0.88     12630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Ewaluacja modelu (sprawdzenie jak się nauczył)\n",
    "\n",
    "\n",
    "print(\"[INFO] evaluating network...\")\n",
    "predictions = model.predict(testX, batch_size=BS)\n",
    "print(classification_report(testY.argmax(axis=1),\n",
    "    predictions.argmax(axis=1), target_names=labelNames))\n",
    "\n",
    "# Wykres funkcji kosztu i dokładności\n",
    "N = np.arange(0, NUM_EPOCHS)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(N, H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(N, H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.plot(N, H.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(N, H.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Training Loss and Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss/Accuracy\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(\"train.png\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMqlNX73oHcw"
   },
   "source": [
    "## Wnioskowanie\n",
    "\n",
    "Jak już mamy nauczony model, to może go użyć do wnioskowania (ang. inference).\n",
    "Wtedy na wejście podajemy zdjęcie znaku, a na wyjściu uzyskujemy informację co to za znak.\n",
    "\n",
    "Uwaga. Proszę utworzyć folder `examples`.\n",
    "\n",
    "Na samym końcu patrzymy co nam wyszło.\n",
    "Z uwagi na ograniczaną liczbę iteracji - \"szału nie ma\", ale i tak znaki o lepszej jakości powinny być rozpoznane poprawnie.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Uo4OyWP9oHcx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading model...\n",
      "[INFO] predicting...\n",
      "[INFO] done...\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from skimage import transform\n",
    "from skimage import exposure\n",
    "from skimage import io\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import random\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Wczytujemy model\n",
    "print(\"[INFO] loading model...\")\n",
    "model = load_model(\"trafficsignnet.model\")\n",
    "# Wczytujemy nazwy klas (ponownie)\n",
    "labelNames = open(\"signnames.csv\").read().strip().split(\"\\n\")[1:]\n",
    "labelNames = [l.split(\",\")[0] for l in labelNames]\n",
    "# Wczytujemy obrazy, mieszamy, wybieramy podzbiór:\n",
    "print(\"[INFO] predicting...\")\n",
    "imagePaths = list(paths.list_images(\"gtsrb/Test\"))\n",
    "random.shuffle(imagePaths)\n",
    "imagePaths = imagePaths[:25]\n",
    "\n",
    "# Pętla po obrazach\n",
    "for (i, imagePath) in enumerate(imagePaths):\n",
    "    # Wczytujemy obraz, skalujemy, wyrównujemy histogram - dokładnie jak wcześniej\n",
    "\n",
    "    image = io.imread(imagePath)\n",
    "    image = transform.resize(image, (32, 32))\n",
    "    image = exposure.equalize_adapthist(image, clip_limit=0.1)\n",
    "    # Skalujemy do wartości 0,1\n",
    "    image = image.astype(\"float32\") / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "\n",
    "    # Przeprowadzamy wnioskowanie\n",
    "    preds = model.predict(image)\n",
    "    # Wybieramy największą odpowedź\n",
    "    j = preds.argmax(axis=1)[0]\n",
    "    label = labelNames[j]\n",
    "\n",
    "    # Wizualizacja i zapis do pliku\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=128)\n",
    "    cv2.putText(image, label, (5, 15), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.45, (0, 0, 255), 2)\n",
    "    p = os.path.sep.join([\"examples\", \"{}.png\".format(i)])\n",
    "    cv2.imwrite(p, image)\n",
    "\n",
    "print(\"[INFO] done...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# import matplotlib\n",
    "# matplotlib.use('TkAgg')\n",
    "# plt.imshow(cv2.imread('train.png'))\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "14_TSR_DCNN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
